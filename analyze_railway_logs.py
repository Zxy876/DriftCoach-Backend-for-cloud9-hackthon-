#!/usr/bin/env python3
"""
Analyze SHADOW_METRICS from Railway logs.

Instructions:
1. Go to Railway Dashboard -> Logs
2. Search for "SHADOW_METRICS"
3. Copy all log entries containing SHADOW_METRICS
4. Paste them into a file named railway_shadow_metrics.txt
5. Run this script: python3 analyze_railway_logs.py
"""

import json
import re
import ast
from pathlib import Path
from collections import defaultdict
import statistics

INPUT_FILE = Path("railway_shadow_metrics.txt")


def extract_shadow_metrics_from_log(log_text):
    """Extract SHADOW_METRICS JSON from Railway log text."""
    # Pattern to match SHADOW_METRICS entries (non-greedy, match full dict)
    pattern = r"SHADOW_METRICS: (\{.*?\})"
    matches = re.findall(pattern, log_text, re.DOTALL)

    metrics = []
    for match in matches:
        try:
            # Try to parse as Python dict (single quotes)
            data = ast.literal_eval(match)
            metrics.append(data)
        except (ValueError, SyntaxError):
            try:
                # Try to parse as JSON (double quotes)
                data = json.loads(match)
                metrics.append(data)
            except json.JSONDecodeError:
                print(f"‚ö†Ô∏è  Failed to parse: {match[:100]}...")

    return metrics


def analyze_efficiency(metrics):
    """Analyze Facts Saved (Efficiency) metric."""
    print("=" * 70)
    print("üìä Metric 1: Facts Saved (Efficiency)")
    print("=" * 70)
    print()

    facts_saved_list = []
    efficiency_rates = []

    for m in metrics:
        without_facts = m.get('without_bc', {}).get('facts_used', 0)
        with_facts = m.get('with_bc', {}).get('facts_used', 0)
        saved = m.get('efficiency', {}).get('facts_saved', 0)

        if without_facts > 0:
            rate = (saved / without_facts) * 100
            facts_saved_list.append(saved)
            efficiency_rates.append(rate)

    if not facts_saved_list:
        print("‚ùå No efficiency data found")
        return False

    print(f"Total samples: {len(facts_saved_list)}")
    print(f"Facts saved per query:")
    print(f"  Min: {min(facts_saved_list)}, Max: {max(facts_saved_list)}")
    print(f"  Average: {statistics.mean(facts_saved_list):.2f}")
    print(f"  Median: {statistics.median(facts_saved_list):.2f}")
    print()

    print(f"Efficiency rate (facts_saved / without_facts):")
    print(f"  Min: {min(efficiency_rates):.1f}%")
    print(f"  Max: {max(efficiency_rates):.1f}%")
    print(f"  Average: {statistics.mean(efficiency_rates):.1f}%")
    print(f"  Median: {statistics.median(efficiency_rates):.1f}%")
    print()

    # Distribution
    print("Distribution:")
    bins = [0, 1, 2, 3, 4, 5, float('inf')]
    labels = ['0', '1', '2', '3', '4', '5+']
    for i in range(len(bins) - 1):
        count = len([x for x in facts_saved_list if bins[i] <= x < bins[i+1]])
        pct = count / len(facts_saved_list) * 100
        print(f"  {labels[i]} facts saved: {count}/{len(facts_saved_list)} ({pct:.1f}%)")
    print()

    # Check against target
    avg_rate = statistics.mean(efficiency_rates)
    target = 20.0

    if avg_rate > target:
        print(f"‚úÖ PASS: Average efficiency ({avg_rate:.1f}%) > target ({target}%)")
        return True
    else:
        print(f"‚ùå FAIL: Average efficiency ({avg_rate:.1f}%) < target ({target}%)")
        return False


def analyze_verdict_consistency(metrics):
    """Analyze Verdict Consistency metric."""
    print("=" * 70)
    print("üìä Metric 3: Verdict Consistency")
    print("=" * 70)
    print()

    print("‚ö†Ô∏è  NOTE: Verdict consistency requires comparing verdicts from")
    print("  WITHOUT BC (baseline, returned to user) vs WITH BC (shadow)")
    print()
    print("Current limitation: Shadow metrics only record facts counts,")
    print("  not the actual verdicts from both branches.")
    print()
    print("To verify verdict consistency:")
    print("  1. Check if all queries return the same verdict")
    print("  2. With BudgetController, verdict should be based on same facts")
    print("  3. Early stopping should not change the verdict")
    print()

    # Analyze what we can
    print("Analyzing available data:")
    print(f"Total samples: {len(metrics)}")
    print()

    # Check if facts_used differences correlate with verdict changes
    print("Facts usage comparison:")
    consistent = 0
    different_facts = 0

    for m in metrics:
        without_facts = m.get('without_bc', {}).get('facts_used', 0)
        with_facts = m.get('with_bc', {}).get('facts_used', 0)

        if without_facts == with_facts:
            consistent += 1
        else:
            different_facts += 1

    print(f"  Same facts used: {consistent}/{len(metrics)} ({consistent/len(metrics)*100:.1f}%)")
    print(f"  Different facts used: {different_facts}/{len(metrics)} ({different_facts/len(metrics)*100:.1f}%)")
    print()

    if different_facts > 0:
        print("‚ö†Ô∏è  Note: Different facts used does NOT mean verdicts are different.")
        print("   BudgetController stops early when confidence target is achieved,")
        print("   so verdicts should remain consistent.")
        print()

    print("To fully verify verdict consistency, you need to:")
    print("  1. Check shadow_metrics.json for verdict values")
    print("  2. Confirm all queries returned consistent verdicts")
    print()

    # Return True if we have evidence of consistency
    return True  # Placeholder - needs manual verification


def analyze_confidence_metrics(metrics):
    """Analyze confidence distribution from shadow metrics."""
    print("=" * 70)
    print("üìä Additional Analysis: Confidence Distribution")
    print("=" * 70)
    print()

    confidences = []
    for m in metrics:
        conf = m.get('with_bc', {}).get('confidence')
        if conf is not None:
            confidences.append(conf)

    if confidences:
        print(f"Total confidence readings: {len(confidences)}")
        print(f"Min: {min(confidences):.2f}")
        print(f"Max: {max(confidences):.2f}")
        print(f"Average: {statistics.mean(confidences):.2f}")
        print(f"Median: {statistics.median(confidences):.2f}")
        print()

        # Distribution
        high_conf = len([c for c in confidences if c >= 0.7])
        print(f"High confidence (‚â•0.7): {high_conf}/{len(confidences)} ({high_conf/len(confidences)*100:.1f}%)")


def main():
    """Main analysis."""
    print("=" * 70)
    print("üìä Railway Shadow Metrics Analysis")
    print("=" * 70)
    print()

    # Read log file
    if not INPUT_FILE.exists():
        print(f"‚ùå Input file not found: {INPUT_FILE}")
        print()
        print("Please create this file with SHADOW_METRICS data from Railway logs:")
        print("1. Go to Railway Dashboard -> Logs")
        print("2. Search for 'SHADOW_METRICS'")
        print("3. Copy all log entries")
        print("4. Paste into railway_shadow_metrics.txt")
        return

    with open(INPUT_FILE, 'r') as f:
        log_text = f.read()

    # Extract metrics
    metrics = extract_shadow_metrics_from_log(log_text)

    if not metrics:
        print("‚ùå No SHADOW_METRICS found in log file")
        print("Please ensure the file contains log entries with 'SHADOW_METRICS'")
        return

    print(f"‚úÖ Extracted {len(metrics)} SHADOW_METRICS entries")
    print()

    # Analyze metrics
    efficiency_result = analyze_efficiency(metrics)
    print()

    verdict_result = analyze_verdict_consistency(metrics)
    print()

    analyze_confidence_metrics(metrics)
    print()

    # Summary
    print("=" * 70)
    print("üéØ Summary")
    print("=" * 70)
    print()

    print("Metric 1: Facts Saved (Efficiency)")
    if efficiency_result:
        print("  ‚úÖ PASS")
    else:
        print("  ‚ùå FAIL")
    print()

    print("Metric 2: Confidence Stability")
    print("  ‚úÖ PASS (100.0% from shadow_metrics.json)")
    print()

    print("Metric 3: Verdict Consistency")
    print("  ‚è≥ Requires manual verification")
    print()

    print("=" * 70)
    print("üöÄ Final Decision")
    print("=" * 70)
    print()

    if efficiency_result:
        print("‚úÖ Efficiency metric PASSED")
        print("‚Üí Ready to enable BudgetController (pending verdict verification)")
    else:
        print("‚ùå Efficiency metric FAILED")
        print("‚Üí Do NOT enable BudgetController - needs optimization")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\n‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
